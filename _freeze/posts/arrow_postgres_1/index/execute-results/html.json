{
  "hash": "8caff980d371f219776ef42a923dd03d",
  "result": {
    "markdown": "---\ntitle: \"Arrrow and PostgreSQL\"\ndescription: \"Comparing simple query performance on different hardware\"\nauthor: \"Thorarinn Jonmundsson\"\ndate: \"2023/03/14\"\n---\n\n\n### Brief background\nThis post is based on a question I asked on [Stack Exchange](https://dba.stackexchange.com/questions/324640/query-performance-on-static-large-postgresql-table). The thread has a lot of details, so I encourage you to read it if you're interested. \n\n### Problem statement\nRecently, I created the following table in Postgres:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nCREATE TABLE protein_snp_assoc (\n  protein_id    int not null,\n  snp_id        int not null,\n  beta          double precision,\n  se            double precision,\n  logp          double precision\n);\n```\n:::\n\n\nSimple and static, meaning that I would bulk insert the contents of roughly 4,800 files, each consisting of ~7,200,000 rows, into the table and then leave it be. and creating an index:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nALTER TABLE protein_snp_assoc\nADD CONSTRAINT protein_snp_assoc_pkey\n    PRIMARY KEY (snp_id, protein_id) WITH (FILLFACTOR = 100);\n```\n:::\n\n\n\n* Describe the solution\n\n\n### Conclusion\nArrow is incredibly fast and easy to set up. Query performance on Arrow was comparable to PostgreSQL on an HDD but once I switched to an SSD, Postgres crushed Arrow. That's not to sayu didn't stand a chance on an SSD. If I want to query with Arrow through an SSL connection, I have to execute scripts with the `ssh` package. Postgres allows me \n\nRecently, I set up a PostgreSQL database at my work to streamline a data pipeline I developed. Before PostgreSQL, the data was stored in ~5,000 files, each about 800 MB and consisting of roughly 7.5 million rows and 13 columns. The resulting table was therefore 35 billion lines or close to 4 TB. \n\n\nIt was a relatively easy task since the data is static; once all data had been inserted into the table, no more \n\nas the table I created was read-onlyhad to put ~5,000 filesSetting up the database was pretty easy since it is a read-only database, meaning that no new data will be added to it. Nevertheless, the size of the table (https://bggj.is/)[My friend] pointed out to \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.table::fread(\"tables/merged_results.txt\") |>\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|drive | n_snps| n_proteins|arrow_parquet               |postgres                 |\n|:-----|------:|----------:|:---------------------------|:------------------------|\n|hdd   |      1|          1|49.3 [44.63 - 48.07]        |18.93 [15.09 - 22.1]     |\n|hdd   |     10|          1|177.48 [164.26 - 176.15]    |188.86 [164.04 - 198.37] |\n|hdd   |     50|          1|766.24 [711.62 - 750.97]    |876.82 [776.22 - 898.57] |\n|hdd   |      1|          2|67.87 [61.48 - 65.68]       |32.33 [24.51 - 38.09]    |\n|hdd   |     10|          2|287.35 [269.54 - 286.23]    |280.9 [265.63 - 297.76]  |\n|hdd   |     10|         10|1089.44 [1038.4 - 1100.23]  |793.89 [764.55 - 823.15] |\n|ssd   |      1|          1|15.81 [14.16 - 15.25]       |0.5 [0.43 - 0.57]        |\n|ssd   |     50|          1|297.73 [264.79 - 316.16]    |12.63 [10.68 - 13.92]    |\n|ssd   |     10|         10|784.37 [763.5 - 779.5]      |14.01 [11.56 - 14.5]     |\n|ssd   |     50|         50|3637.89 [3250.68 - 3887.82] |227.33 [217.28 - 233.5]  |\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}